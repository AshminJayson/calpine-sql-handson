{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34b3645f",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bf20a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Linear Regression using scikit-learn\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(0)\n",
    "X = 2 * np.random.rand(200, 1)\n",
    "y = 4 + 3 * X + np.random.randn(200, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9065db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b614af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "r2_score = model.score(X_test, y_test)\n",
    "print(f\"R^2 Score: {r2_score}\")\n",
    "mae = np.mean(np.abs(y_test - y_pred))\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "\n",
    "# What is MSE, R^2 Score, and MAE\n",
    "# MSE (Mean Squared Error) measures the average of the squares of the errors—that is, the average squared difference between the estimated values and the actual value.\n",
    "# R^2 Score (Coefficient of Determination) indicates the proportion of the variance in the dependent variable that is predictable from the independent variable(s). An R^2 of 1 indicates that the regression predictions perfectly fit the data.\n",
    "# MAE (Mean Absolute Error) measures the average magnitude of the errors in a set of predictions, without considering their direction. It’s the average over the test sample of the absolute differences between predicted values and actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ec0b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "\n",
    "plt.scatter(X, y, color='blue', label='Data points')\n",
    "plt.plot(X_test, y_pred, color='red', linewidth=2, label='Regression line')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Linear Regression using scikit-learn')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e02fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4fbecd",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efff0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression using Scikit-learn\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e73d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and show the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f447960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform logistic regression on the iris dataset\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3512d616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4874bd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset for logistic regression with two classes and plot as well\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(n_samples=200, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1,\n",
    "                            random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Create and train the Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "# Plot the decision boundary\n",
    "x_min, x_max = X_test[:, 0].min() - 1, X_test[:, 0].max() + 1\n",
    "y_min, y_max = X_test[:, 1].min() - 1, X_test[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
    "                     np.arange(y_min, y_max, 0.01))\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, edgecolors='k', marker='o')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Logistic Regression Decision Boundary (Test Set)')\n",
    "plt.show()\n",
    "\n",
    "# Fetch values for precision, recall, f1-score, and support from classification report\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "\n",
    "\n",
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc29f14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain the accuracy, classification report, and confusion matrix\n",
    "# Accuracy is the ratio of correctly predicted observations to the total observations. It gives a quick overview of how well the model is performing.\n",
    "# The Classification Report provides a detailed breakdown of precision, recall, F1-score, and support for each class. Precision is the ratio of correctly predicted positive observations to the total predicted positives. Recall (sensitivity) is the ratio of correctly predicted positive observations to all actual positives. F1-score is the weighted average of Precision and Recall.\n",
    "# The Confusion Matrix is a table that is often used to describe the performance of a classification model. It shows the true positives, true negatives, false positives, and false negatives, allowing for a more detailed analysis of the model's performance across different classes.\n",
    "\n",
    "# What is precision, recall, F1-score, and support\n",
    "# Precision is the ratio of correctly predicted positive observations to the total predicted positives. It indicates how many of the predicted positive cases were actually positive.\n",
    "# Recall (sensitivity) is the ratio of correctly predicted positive observations to all actual positives. It indicates how many of the actual positive cases were captured by the model.\n",
    "# F1-score is the weighted average of Precision and Recall. It is a useful metric when you need to balance both precision and recall, especially in cases of imbalanced datasets.\n",
    "# Support is the number of actual occurrences of each class in the dataset. It indicates how many samples belong to each class.\n",
    "\n",
    "# Precision vs Recall\n",
    "# Precision and Recall are two important metrics used to evaluate the performance of classification models, especially in scenarios where the classes are imbalanced.\n",
    "# Precision focuses on the accuracy of the positive predictions made by the model. High precision means that when the model predicts a positive class, it is usually correct.\n",
    "# Recall, on the other hand, focuses on the model's ability to capture all actual positive instances. High recall means that the model is able to identify most of the positive cases in the dataset.\n",
    "\n",
    "# Example\n",
    "# Consider a medical test for a disease. If the test has high precision, it means that when it indicates a person has the disease, it is very likely to be correct (few false positives). If the test has high recall, it means that it successfully identifies most of the people who actually have the disease (few false negatives). Depending on the context, one may prioritize precision over recall or vice versa.\n",
    "\n",
    "# Decoding the confusion matrix\n",
    "# In a confusion matrix, each row represents the instances in a predicted class while each column represents the instances in an actual class (or vice versa). The diagonal elements represent the number of correct predictions for each class, while the off-diagonal elements represent misclassifications. By analyzing the confusion matrix, one can identify which classes are being confused with each other and assess the model's performance in more detail.\n",
    "\n",
    "# Example\n",
    "# Confusion Matrix:\n",
    "# [[10  0  0]\n",
    "#  [ 0  9  1]\n",
    "#  [ 0  2  8]]\n",
    "# In this example, for class 0, there were 10 correct predictions and no misclassifications. For class 1, there were 9 correct predictions and 1 misclassification as class 2. For class 2, there were 8 correct predictions and 2 misclassifications as class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185d90c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding Entropy and Information Gain in Decision Trees\n",
    "\n",
    "# Entropy is a measure of uncertainty or impurity in a dataset. In the context of decision trees, it quantifies the disorder or randomness in the class labels of the data points. A dataset with high entropy has a mix of different classes, while a dataset with low entropy has mostly one class.\n",
    "# Information Gain is a metric used to measure the effectiveness of a feature in classifying the training data. It quantifies the reduction in entropy achieved by splitting the dataset based on a particular feature. The feature that provides the highest information gain is chosen for the split at each node in the decision tree.\n",
    "\n",
    "\n",
    "# Create a more comprehensive dataset for demonstration\n",
    "ex_dataset = [\n",
    "    (\"Adult-Y\", \"Rich\", \"Yes\"),\n",
    "    (\"Adult-Y\", \"Low\", \"No\"), \n",
    "    (\"Adult-Y\", \"Medium\", \"No\"),\n",
    "    (\"Adult-Y\", \"Rich\", \"Yes\"),\n",
    "    (\"Middle-aged\", \"Rich\", \"Yes\"),\n",
    "    (\"Middle-aged\", \"Low\", \"No\"),\n",
    "    (\"Middle-aged\", \"Medium\", \"Yes\"),\n",
    "    (\"Middle-aged\", \"Medium\", \"No\"),\n",
    "    (\"Middle-aged\", \"Low\", \"No\"),\n",
    "    (\"Senior\", \"Rich\", \"Yes\"),\n",
    "    (\"Senior\", \"Low\", \"No\"),\n",
    "    (\"Senior\", \"Medium\", \"Yes\"),\n",
    "    (\"Senior\", \"Rich\", \"Yes\")\n",
    "]\n",
    "\n",
    "# ex_dataset = [i for i in ex_dataset if i[1] == 'Medium']\n",
    "ex_dataset.sort(key=lambda x: x[0])\n",
    "# Print as table\n",
    "print(f\"{'AgeGroup':<12} {'IncomeLevel':<12} {'OwnsCar':<8}\")\n",
    "print(\"-\" * 32)\n",
    "for row in ex_dataset[1:]:\n",
    "    print(f\"{row[0]:<12} {row[1]:<12} {row[2]:<8}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1e9d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Trees\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac873582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5c0591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the Decision Tree model\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eec131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision tree\n",
    "from sklearn.tree import plot_tree\n",
    "plt.figure(figsize=(12,8))\n",
    "plot_tree(model, filled=True, feature_names=iris.feature_names, class_names=iris.target\n",
    "                     .astype(str))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcef151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest and ensemble learning\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca3f4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Create and train the Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "model.fit(X_train, y_train)\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608fcb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation and comparison on ROC, AUC, cross-validation\n",
    "\n",
    "# Since we already have the iris dataset loaded and split, we can proceed directly\n",
    "# Create and train the Logistic Regression model\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities on the test set\n",
    "y_probs = model.predict_proba(X_test)\n",
    "\n",
    "# For multiclass classification, calculate ROC AUC using 'ovr' (one-vs-rest) strategy\n",
    "auc = roc_auc_score(y_test, y_probs, multi_class='ovr')\n",
    "print(f\"ROC AUC Score (One-vs-Rest): {auc}\")\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(model, X, y, cv=5, scoring='roc_auc_ovr')\n",
    "print(f\"Cross-Validation ROC AUC Scores: {cv_scores}\")\n",
    "print(f\"Mean Cross-Validation ROC AUC Score: {np.mean(cv_scores)}\")\n",
    "\n",
    "# Optional: Plot ROC curves for each class\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(len(iris.target_names)):\n",
    "    # Convert to binary classification (one class vs rest)\n",
    "    y_test_binary = (y_test == i).astype(int)\n",
    "    y_probs_binary = y_probs[:, i]\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test_binary, y_probs_binary)\n",
    "    auc_class = roc_auc_score(y_test_binary, y_probs_binary)\n",
    "    \n",
    "    plt.plot(fpr, tpr, label=f'{iris.target_names[i]} (AUC = {auc_class:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for Multiclass Classification')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bfe79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ROC (Receiver Operating Characteristic) curves plotted above represent the performance of the Logistic Regression model for each class in a one-vs-rest manner. Each curve shows the trade-off between the True Positive Rate (sensitivity) and the False Positive Rate for different threshold values. The area under each curve (AUC) quantifies the overall ability of the model to discriminate between the positive class and the negative class. An AUC of 1 indicates perfect classification, while an AUC of 0.5 suggests no discriminative power (random guessing). The closer the ROC curve is to the top-left corner, the better the model's performance for that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3179b1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['liblinear', 'lbfgs'],\n",
    "    'max_iter': [100, 200, 300]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='roc_auc_ovr')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"Best Cross-Validation ROC AUC Score: {grid_search.best_score_}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
