{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34b3645f",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bf20a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Linear Regression using scikit-learn\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(0)\n",
    "X = 2 * np.random.rand(200, 1)\n",
    "y = 4 + 3 * X + np.random.randn(200, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9065db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b614af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "r2_score = model.score(X_test, y_test)\n",
    "print(f\"R^2 Score: {r2_score}\")\n",
    "mae = np.mean(np.abs(y_test - y_pred))\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "\n",
    "# What is MSE, R^2 Score, and MAE\n",
    "# MSE (Mean Squared Error) measures the average of the squares of the errors—that is, the average squared difference between the estimated values and the actual value.\n",
    "# R^2 Score (Coefficient of Determination) indicates the proportion of the variance in the dependent variable that is predictable from the independent variable(s). An R^2 of 1 indicates that the regression predictions perfectly fit the data.\n",
    "# MAE (Mean Absolute Error) measures the average magnitude of the errors in a set of predictions, without considering their direction. It’s the average over the test sample of the absolute differences between predicted values and actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ec0b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "\n",
    "plt.scatter(X, y, color='blue', label='Data points')\n",
    "plt.plot(X_test, y_pred, color='red', linewidth=2, label='Regression line')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Linear Regression using scikit-learn')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4fbecd",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efff0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression using Scikit-learn\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e73d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and show the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f447960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform logistic regression on the iris dataset\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3512d616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc29f14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain the accuracy, classification report, and confusion matrix\n",
    "# Accuracy is the ratio of correctly predicted observations to the total observations. It gives a quick overview of how well the model is performing.\n",
    "# The Classification Report provides a detailed breakdown of precision, recall, F1-score, and support for each class. Precision is the ratio of correctly predicted positive observations to the total predicted positives. Recall (sensitivity) is the ratio of correctly predicted positive observations to all actual positives. F1-score is the weighted average of Precision and Recall.\n",
    "# The Confusion Matrix is a table that is often used to describe the performance of a classification model. It shows the true positives, true negatives, false positives, and false negatives, allowing for a more detailed analysis of the model's performance across different classes.\n",
    "\n",
    "# What is precision, recall, F1-score, and support\n",
    "# Precision is the ratio of correctly predicted positive observations to the total predicted positives. It indicates how many of the predicted positive cases were actually positive.\n",
    "# Recall (sensitivity) is the ratio of correctly predicted positive observations to all actual positives. It indicates how many of the actual positive cases were captured by the model.\n",
    "# F1-score is the weighted average of Precision and Recall. It is a useful metric when you need to balance both precision and recall, especially in cases of imbalanced datasets.\n",
    "# Support is the number of actual occurrences of each class in the dataset. It indicates how many samples belong to each class.\n",
    "\n",
    "# Decoding the confusion matrix\n",
    "# In a confusion matrix, each row represents the instances in a predicted class while each column represents the instances in an actual class (or vice versa). The diagonal elements represent the number of correct predictions for each class, while the off-diagonal elements represent misclassifications. By analyzing the confusion matrix, one can identify which classes are being confused with each other and assess the model's performance in more detail.\n",
    "\n",
    "# Example\n",
    "# Confusion Matrix:\n",
    "# [[10  0  0]\n",
    "#  [ 0  9  1]\n",
    "#  [ 0  2  8]]\n",
    "# In this example, for class 0, there were 10 correct predictions and no misclassifications. For class 1, there were 9 correct predictions and 1 misclassification as class 2. For class 2, there were 8 correct predictions and 2 misclassifications as class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1e9d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Trees\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac873582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5c0591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the Decision Tree model\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcef151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest and ensemble learning\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca3f4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Create and train the Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "model.fit(X_train, y_train)\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608fcb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation and comparison on ROC, AUC, cross-validation\n",
    "\n",
    "# Since we already have the iris dataset loaded and split, we can proceed directly\n",
    "# Create and train the Logistic Regression model\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities on the test set\n",
    "y_probs = model.predict_proba(X_test)\n",
    "\n",
    "# For multiclass classification, calculate ROC AUC using 'ovr' (one-vs-rest) strategy\n",
    "auc = roc_auc_score(y_test, y_probs, multi_class='ovr')\n",
    "print(f\"ROC AUC Score (One-vs-Rest): {auc}\")\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(model, X, y, cv=5, scoring='roc_auc_ovr')\n",
    "print(f\"Cross-Validation ROC AUC Scores: {cv_scores}\")\n",
    "print(f\"Mean Cross-Validation ROC AUC Score: {np.mean(cv_scores)}\")\n",
    "\n",
    "# Optional: Plot ROC curves for each class\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(len(iris.target_names)):\n",
    "    # Convert to binary classification (one class vs rest)\n",
    "    y_test_binary = (y_test == i).astype(int)\n",
    "    y_probs_binary = y_probs[:, i]\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test_binary, y_probs_binary)\n",
    "    auc_class = roc_auc_score(y_test_binary, y_probs_binary)\n",
    "    \n",
    "    plt.plot(fpr, tpr, label=f'{iris.target_names[i]} (AUC = {auc_class:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for Multiclass Classification')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bfe79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ROC (Receiver Operating Characteristic) curves plotted above represent the performance of the Logistic Regression model for each class in a one-vs-rest manner. Each curve shows the trade-off between the True Positive Rate (sensitivity) and the False Positive Rate for different threshold values. The area under each curve (AUC) quantifies the overall ability of the model to discriminate between the positive class and the negative class. An AUC of 1 indicates perfect classification, while an AUC of 0.5 suggests no discriminative power (random guessing). The closer the ROC curve is to the top-left corner, the better the model's performance for that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3179b1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['liblinear', 'lbfgs'],\n",
    "    'max_iter': [100, 200, 300]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='roc_auc_ovr')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"Best Cross-Validation ROC AUC Score: {grid_search.best_score_}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
