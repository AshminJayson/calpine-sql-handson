{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24fbbfa4",
   "metadata": {},
   "source": [
    "# Maths in Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dcd156",
   "metadata": {},
   "source": [
    "## Introduction & Importance of Mathematics in Data Science\n",
    "\n",
    "Mathematics forms the backbone of data science, providing the theoretical foundation and practical tools necessary for extracting meaningful insights from data. In today's data-driven world, a solid understanding of mathematical concepts is essential for any data scientist.\n",
    "\n",
    "### Role of Mathematics in AI/ML\n",
    "\n",
    "Mathematics plays a crucial role in artificial intelligence and machine learning:\n",
    "\n",
    "- **Algorithm Development**: Mathematical principles guide the creation of learning algorithms\n",
    "- **Model Optimization**: Calculus helps in finding optimal parameters through gradient descent\n",
    "- **Pattern Recognition**: Linear algebra enables computers to identify patterns in high-dimensional data\n",
    "- **Uncertainty Quantification**: Probability theory helps models make predictions with confidence intervals\n",
    "- **Performance Evaluation**: Statistics provides metrics to assess model accuracy and reliability\n",
    "\n",
    "### Overview of Key Mathematical Areas in Data Workflows\n",
    "\n",
    "#### Linear Algebra\n",
    "- **Vectors and Matrices**: Foundation for representing and manipulating data\n",
    "- **Eigenvalues and Eigenvectors**: Essential for dimensionality reduction (PCA)\n",
    "- **Matrix Operations**: Core computations in neural networks and transformations\n",
    "\n",
    "#### Calculus\n",
    "- **Derivatives**: Used in optimization algorithms like gradient descent\n",
    "- **Partial Derivatives**: Critical for backpropagation in neural networks\n",
    "- **Chain Rule**: Enables efficient computation of gradients in complex models\n",
    "\n",
    "#### Probability Theory\n",
    "- **Distributions**: Models uncertainty in data and predictions\n",
    "- **Bayes' Theorem**: Foundation for probabilistic machine learning\n",
    "- **Random Variables**: Represents uncertain outcomes in statistical models\n",
    "\n",
    "#### Statistics\n",
    "- **Descriptive Statistics**: Summarizes and describes data characteristics\n",
    "- **Inferential Statistics**: Makes predictions and draws conclusions from samples\n",
    "- **Hypothesis Testing**: Validates assumptions and model performance\n",
    "- **Regression Analysis**: Models relationships between variables\n",
    "\n",
    "These mathematical foundations work together to enable data scientists to build robust, interpretable, and effective analytical solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8368a5",
   "metadata": {},
   "source": [
    "## Linear Algebra Basics\n",
    "\n",
    "Linear algebra is fundamental to data science, providing the mathematical framework for representing and manipulating data efficiently.\n",
    "\n",
    "### Scalars, Vectors, and Matrices\n",
    "\n",
    "#### Scalars\n",
    "A **scalar** is a single number that represents magnitude without direction. In data science, scalars often represent:\n",
    "- Individual data points\n",
    "- Model parameters (learning rate, regularization coefficient)\n",
    "- Performance metrics (accuracy, loss)\n",
    "\n",
    "#### Vectors\n",
    "A **vector** is an ordered list of numbers, representing both magnitude and direction. Vectors are used for:\n",
    "- Representing individual data samples as feature vectors\n",
    "- Storing model weights and biases\n",
    "- Encoding word embeddings in NLP\n",
    "\n",
    "**Example**: A customer record as a vector: `[age, income, credit_score] = [35, 65000, 720]`\n",
    "\n",
    "#### Matrices\n",
    "A **matrix** is a 2D array of numbers arranged in rows and columns. Matrices enable:\n",
    "- Storing entire datasets (rows = samples, columns = features)\n",
    "- Representing transformations and neural network layers\n",
    "- Efficient batch processing of data\n",
    "\n",
    "### Matrix Operations\n",
    "\n",
    "#### Addition\n",
    "Matrices of the same dimensions can be added element-wise:\n",
    "```\n",
    "A + B = [a₁₁ + b₁₁  a₁₂ + b₁₂]\n",
    "    [a₂₁ + b₂₁  a₂₂ + b₂₂]\n",
    "```\n",
    "\n",
    "#### Multiplication\n",
    "- **Scalar multiplication**: Multiply each element by a scalar\n",
    "- **Matrix multiplication**: Combine matrices following the dot product rule\n",
    "- Essential for neural network forward passes and data transformations\n",
    "\n",
    "#### Transpose\n",
    "Flipping a matrix along its diagonal, converting rows to columns:\n",
    "- **Use case**: Converting row vectors to column vectors\n",
    "- **Notation**: A^T or A'\n",
    "\n",
    "#### Inverse\n",
    "The inverse of matrix A (denoted A⁻¹) satisfies: A × A⁻¹ = I (identity matrix)\n",
    "- **Applications**: Solving linear systems, computing pseudo-inverses in regression\n",
    "\n",
    "### Applications in Data Representation\n",
    "\n",
    "#### Dataset Representation\n",
    "```\n",
    "Dataset Matrix:\n",
    "     [feature₁  feature₂  feature₃]\n",
    "X = [   x₁₁      x₁₂      x₁₃   ]  ← Sample 1\n",
    "    [   x₂₁      x₂₂      x₂₃   ]  ← Sample 2\n",
    "    [   x₃₁      x₃₂      x₃₃   ]  ← Sample 3\n",
    "```\n",
    "\n",
    "#### Dimensionality Reduction\n",
    "- **PCA (Principal Component Analysis)**: Uses eigenvalues and eigenvectors to find principal components\n",
    "- **SVD (Singular Value Decomposition)**: Decomposes matrices for compression and noise reduction\n",
    "\n",
    "#### Neural Networks\n",
    "- **Weight matrices**: Store learned parameters between layers\n",
    "- **Batch processing**: Process multiple samples simultaneously using matrix operations\n",
    "- **Activation functions**: Applied element-wise to matrix outputs\n",
    "\n",
    "#### Distance Calculations\n",
    "- **Euclidean distance**: Using vector norms to measure similarity\n",
    "- **Cosine similarity**: Measuring angle between vectors for recommendation systems\n",
    "\n",
    "Linear algebra operations form the computational backbone of modern machine learning frameworks, enabling efficient processing of large datasets and complex model architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc58b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scalars\n",
    "print(\"=== SCALARS ===\")\n",
    "learning_rate = 0.01\n",
    "accuracy = 0.95\n",
    "print(f\"Learning rate (scalar): {learning_rate}\")\n",
    "print(f\"Model accuracy (scalar): {accuracy}\")\n",
    "\n",
    "# Vectors\n",
    "print(\"\\n=== VECTORS ===\")\n",
    "# Customer data as a vector [age, income, credit_score]\n",
    "customer_vector = np.array([35, 65000, 720])\n",
    "print(f\"Customer vector: {customer_vector}\")\n",
    "\n",
    "# Feature vector for a house [bedrooms, bathrooms, sqft, price]\n",
    "house_vector = np.array([3, 2, 1500, 250000])\n",
    "print(f\"House vector: {house_vector}\")\n",
    "\n",
    "# Matrices\n",
    "print(\"\\n=== MATRICES ===\")\n",
    "# Dataset matrix - 4 customers with 3 features each\n",
    "dataset_matrix = np.array([\n",
    "    [25, 45000, 680],  # Customer 1\n",
    "    [35, 65000, 720],  # Customer 2\n",
    "    [45, 85000, 750],  # Customer 3\n",
    "    [28, 52000, 690]   # Customer 4\n",
    "])\n",
    "print(\"Dataset matrix (4 customers × 3 features):\")\n",
    "print(dataset_matrix)\n",
    "\n",
    "# Matrix Operations\n",
    "print(\"\\n=== MATRIX OPERATIONS ===\")\n",
    "\n",
    "# Matrix addition\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[5, 6], [7, 8]])\n",
    "print(f\"Matrix A:\\n{A}\")\n",
    "print(f\"Matrix B:\\n{B}\")\n",
    "print(f\"A + B:\\n{A + B}\")\n",
    "\n",
    "# Scalar multiplication\n",
    "scalar = 2\n",
    "print(f\"\\nScalar multiplication (A × {scalar}):\\n{scalar * A}\")\n",
    "\n",
    "# Matrix multiplication\n",
    "C = np.array([[1, 2], [3, 4]])\n",
    "D = np.array([[2, 0], [1, 2]])\n",
    "print(f\"\\nMatrix C:\\n{C}\")\n",
    "print(f\"Matrix D:\\n{D}\")\n",
    "print(f\"C × D:\\n{np.dot(C, D)}\")\n",
    "\n",
    "# Transpose\n",
    "print(f\"\\nTranspose of dataset_matrix:\")\n",
    "print(f\"Original shape: {dataset_matrix.shape}\")\n",
    "print(f\"Transposed shape: {dataset_matrix.T.shape}\")\n",
    "print(dataset_matrix.T)\n",
    "\n",
    "# Matrix inverse (for square matrices)\n",
    "square_matrix = np.array([[2, 1], [1, 1]])\n",
    "inverse_matrix = np.linalg.inv(square_matrix)\n",
    "print(f\"\\nSquare matrix:\\n{square_matrix}\")\n",
    "print(f\"Its inverse:\\n{inverse_matrix}\")\n",
    "print(f\"Verification (should be identity matrix):\\n{np.dot(square_matrix, inverse_matrix)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7aa66c",
   "metadata": {},
   "source": [
    "## Calculus for Data Science\n",
    "\n",
    "Calculus is the mathematical study of continuous change, providing essential tools for optimization and understanding how functions behave. In data science, calculus enables us to find optimal parameters, minimize error functions, and understand the rate of change in our models.\n",
    "\n",
    "### Functions, Limits, and Derivatives\n",
    "\n",
    "#### Functions in Data Science\n",
    "A **function** maps input values to output values, fundamental in modeling relationships between variables:\n",
    "- **Cost functions**: Measure prediction error (e.g., Mean Squared Error)\n",
    "- **Activation functions**: Transform inputs in neural networks (sigmoid, ReLU)\n",
    "- **Probability density functions**: Describe data distributions\n",
    "\n",
    "#### Limits\n",
    "**Limits** describe the behavior of functions as inputs approach specific values:\n",
    "- Foundation for defining derivatives and continuity\n",
    "- Used in convergence analysis of optimization algorithms\n",
    "- Essential for understanding function behavior at boundaries\n",
    "\n",
    "#### Derivatives\n",
    "The **derivative** measures the instantaneous rate of change of a function:\n",
    "- **Geometric interpretation**: Slope of the tangent line at a point\n",
    "- **Physical interpretation**: Rate of change (velocity, acceleration)\n",
    "- **Notation**: f'(x), df/dx, or ∂f/∂x\n",
    "\n",
    "**Key derivative rules**:\n",
    "- Power rule: d/dx(x^n) = nx^(n-1)\n",
    "- Chain rule: d/dx[f(g(x))] = f'(g(x)) × g'(x)\n",
    "- Product rule: d/dx[f(x)g(x)] = f'(x)g(x) + f(x)g'(x)\n",
    "\n",
    "### Partial Derivatives and Gradients\n",
    "\n",
    "#### Partial Derivatives\n",
    "When functions depend on multiple variables, **partial derivatives** measure the rate of change with respect to one variable while keeping others constant:\n",
    "- **Notation**: ∂f/∂x, ∂f/∂y\n",
    "- **Application**: Finding how each feature affects the prediction error\n",
    "\n",
    "**Example**: For f(x,y) = x² + 3xy + y²\n",
    "- ∂f/∂x = 2x + 3y\n",
    "- ∂f/∂y = 3x + 2y\n",
    "\n",
    "#### Gradients\n",
    "The **gradient** is a vector containing all partial derivatives of a function:\n",
    "- **Notation**: ∇f = [∂f/∂x₁, ∂f/∂x₂, ..., ∂f/∂xₙ]\n",
    "- **Geometric meaning**: Points in the direction of steepest increase\n",
    "- **Magnitude**: Indicates the rate of steepest increase\n",
    "\n",
    "### Applications in Optimization\n",
    "\n",
    "#### Cost Functions\n",
    "Cost functions quantify prediction errors and guide model training:\n",
    "\n",
    "**Mean Squared Error (MSE)**:\n",
    "```\n",
    "J(θ) = (1/2m) Σᵢ(hθ(xᵢ) - yᵢ)²\n",
    "```\n",
    "Where:\n",
    "- θ represents model parameters\n",
    "- hθ(x) is the prediction function\n",
    "- m is the number of training examples\n",
    "\n",
    "#### Gradient Descent Algorithm\n",
    "**Gradient descent** uses derivatives to find function minima iteratively:\n",
    "\n",
    "1. **Calculate gradient**: ∇J(θ) = [∂J/∂θ₁, ∂J/∂θ₂, ..., ∂J/∂θₙ]\n",
    "2. **Update parameters**: θ = θ - α∇J(θ)\n",
    "3. **Repeat** until convergence\n",
    "\n",
    "Where α is the learning rate controlling step size.\n",
    "\n",
    "#### Types of Gradient Descent\n",
    "- **Batch Gradient Descent**: Uses entire dataset for each update\n",
    "- **Stochastic Gradient Descent (SGD)**: Uses one sample at a time\n",
    "- **Mini-batch Gradient Descent**: Uses small batches of samples\n",
    "\n",
    "#### Optimization Challenges\n",
    "- **Local minima**: Gradient descent may get stuck in local optima\n",
    "- **Saddle points**: Points where gradient is zero but not optimal\n",
    "- **Learning rate selection**: Too large causes oscillation; too small causes slow convergence\n",
    "- **Vanishing/exploding gradients**: Gradients become too small or large in deep networks\n",
    "\n",
    "#### Advanced Optimization Techniques\n",
    "- **Momentum**: Accumulates gradients to overcome local minima\n",
    "- **Adam optimizer**: Adapts learning rates for each parameter\n",
    "- **RMSprop**: Scales gradients by recent gradient magnitudes\n",
    "\n",
    "#### Chain Rule in Backpropagation\n",
    "In neural networks, the **chain rule** enables efficient gradient computation:\n",
    "- Gradients flow backward through network layers\n",
    "- Each layer's gradient depends on subsequent layers' gradients\n",
    "- Enables training of deep neural networks\n",
    "\n",
    "Calculus provides the mathematical foundation for training machine learning models, enabling algorithms to automatically learn optimal parameters from data through systematic optimization processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1313d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculus for Data Science - Practical Examples\n",
    "\n",
    "print(\"=== FUNCTIONS IN DATA SCIENCE ===\")\n",
    "\n",
    "# Define common functions used in data science\n",
    "def linear_function(x, w, b):\n",
    "    \"\"\"Linear function: f(x) = wx + b\"\"\"\n",
    "    return w * x + b\n",
    "\n",
    "def quadratic_cost(y_true, y_pred):\n",
    "    \"\"\"Mean Squared Error cost function\"\"\"\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"ReLU activation function\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Example data\n",
    "x_data = np.linspace(-5, 5, 100)\n",
    "y_linear = linear_function(x_data, 2, 1)\n",
    "y_sigmoid = sigmoid(x_data)\n",
    "y_relu = relu(x_data)\n",
    "\n",
    "print(\"Functions defined: linear, quadratic_cost, sigmoid, relu\")\n",
    "\n",
    "print(\"\\n=== DERIVATIVES AND GRADIENTS ===\")\n",
    "\n",
    "# Analytical derivatives\n",
    "def linear_derivative(x, w):\n",
    "    \"\"\"Derivative of linear function with respect to x\"\"\"\n",
    "    return w\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"Derivative of sigmoid function\"\"\"\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def cost_gradient_w(x, y_true, w, b):\n",
    "    \"\"\"Partial derivative of MSE with respect to weight w\"\"\"\n",
    "    y_pred = linear_function(x, w, b)\n",
    "    return -2 * np.mean(x * (y_true - y_pred))\n",
    "\n",
    "def cost_gradient_b(x, y_true, w, b):\n",
    "    \"\"\"Partial derivative of MSE with respect to bias b\"\"\"\n",
    "    y_pred = linear_function(x, w, b)\n",
    "    return -2 * np.mean(y_true - y_pred)\n",
    "\n",
    "# Example gradient calculation\n",
    "x_sample = np.array([1, 2, 3, 4, 5])\n",
    "y_sample = np.array([3, 5, 7, 9, 11])  # y = 2x + 1 with noise\n",
    "w_current = 1.5\n",
    "b_current = 0.5\n",
    "\n",
    "grad_w = cost_gradient_w(x_sample, y_sample, w_current, b_current)\n",
    "grad_b = cost_gradient_b(x_sample, y_sample, w_current, b_current)\n",
    "\n",
    "print(f\"Current parameters: w={w_current}, b={b_current}\")\n",
    "print(f\"Gradient w.r.t. weight: {grad_w:.4f}\")\n",
    "print(f\"Gradient w.r.t. bias: {grad_b:.4f}\")\n",
    "\n",
    "print(\"\\n=== GRADIENT DESCENT OPTIMIZATION ===\")\n",
    "\n",
    "# Implement gradient descent for linear regression\n",
    "def gradient_descent_step(x, y, w, b, learning_rate):\n",
    "    \"\"\"Single step of gradient descent\"\"\"\n",
    "    grad_w = cost_gradient_w(x, y, w, b)\n",
    "    grad_b = cost_gradient_b(x, y, w, b)\n",
    "    \n",
    "    w_new = w - learning_rate * grad_w\n",
    "    b_new = b - learning_rate * grad_b\n",
    "    \n",
    "    return w_new, b_new\n",
    "\n",
    "# Run gradient descent\n",
    "w, b = 0.0, 0.0  # Initialize parameters\n",
    "costs = []\n",
    "weights = []\n",
    "biases = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    # Calculate current cost\n",
    "    y_pred = linear_function(x_sample, w, b)\n",
    "    cost = quadratic_cost(y_sample, y_pred)\n",
    "    costs.append(cost)\n",
    "    weights.append(w)\n",
    "    biases.append(b)\n",
    "    \n",
    "    # Update parameters\n",
    "    w, b = gradient_descent_step(x_sample, y_sample, w, b, learning_rate)\n",
    "\n",
    "print(f\"Final parameters after 100 iterations:\")\n",
    "print(f\"Weight: {w:.4f} (true value: 2.0)\")\n",
    "print(f\"Bias: {b:.4f} (true value: 1.0)\")\n",
    "print(f\"Final cost: {costs[-1]:.6f}\")\n",
    "\n",
    "print(\"\\n=== VISUALIZATION SETUP ===\")\n",
    "\n",
    "# Create visualizations\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Different function types\n",
    "ax1.plot(x_data, y_linear, label='Linear: f(x) = 2x + 1', linewidth=2)\n",
    "ax1.plot(x_data, y_sigmoid, label='Sigmoid: σ(x)', linewidth=2)\n",
    "ax1.plot(x_data, y_relu, label='ReLU: max(0, x)', linewidth=2)\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('f(x)')\n",
    "ax1.set_title('Common Functions in Data Science')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Functions and their derivatives\n",
    "# Calculate derivatives for all functions\n",
    "linear_derivatives = np.full_like(x_data, 2)  # Derivative of 2x + 1 is constant 2\n",
    "relu_derivatives = np.where(x_data > 0, 1, 0)  # Derivative of ReLU\n",
    "\n",
    "# Plot functions\n",
    "ax2.plot(x_data, y_sigmoid, label='Sigmoid: σ(x)', linewidth=2, color='blue')\n",
    "ax2.plot(x_data, y_linear, label='Linear: 2x + 1', linewidth=2, color='orange')\n",
    "ax2.plot(x_data, y_relu, label='ReLU: max(0, x)', linewidth=2, color='green')\n",
    "\n",
    "# Plot derivatives with dashed lines\n",
    "ax2.plot(x_data, sigmoid_derivatives, label=\"Sigmoid derivative: σ'(x)\", linewidth=2, linestyle='--', color='blue', alpha=0.7)\n",
    "ax2.plot(x_data, linear_derivatives, label=\"Linear derivative: 2\", linewidth=2, linestyle='--', color='orange', alpha=0.7)\n",
    "ax2.plot(x_data, relu_derivatives, label=\"ReLU derivative\", linewidth=2, linestyle='--', color='green', alpha=0.7)\n",
    "\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.set_title('Functions and Their Derivatives')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Cost function during training\n",
    "ax3.plot(costs, linewidth=2, color='red')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Cost (MSE)')\n",
    "ax3.set_title('Cost Function During Gradient Descent')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Parameter evolution\n",
    "ax4.plot(weights, label=f'Weight (final: {w:.3f})', linewidth=2)\n",
    "ax4.plot(biases, label=f'Bias (final: {b:.3f})', linewidth=2)\n",
    "ax4.axhline(y=2.0, color='red', linestyle='--', alpha=0.7, label='True weight: 2.0')\n",
    "ax4.axhline(y=1.0, color='orange', linestyle='--', alpha=0.7, label='True bias: 1.0')\n",
    "ax4.set_xlabel('Epoch')\n",
    "ax4.set_ylabel('Parameter Value')\n",
    "ax4.set_title('Parameter Evolution During Training')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== PRACTICAL INSIGHTS ===\")\n",
    "print(\"• Derivatives tell us how to adjust parameters to minimize cost\")\n",
    "print(\"• Gradient descent uses calculus to automatically learn optimal parameters\")\n",
    "print(\"• The learning rate controls how big steps we take in parameter space\")\n",
    "print(\"• Chain rule enables backpropagation in neural networks\")\n",
    "print(\"• Different activation functions have different derivative properties\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc5ec41",
   "metadata": {},
   "source": [
    "## Regression Analysis\n",
    "\n",
    "Regression analysis is a fundamental statistical method in data science that models the relationship between variables, enabling us to make predictions and understand how different factors influence outcomes.\n",
    "\n",
    "### Simple Linear Regression\n",
    "\n",
    "**Simple linear regression** models the relationship between two variables: one independent variable (predictor) and one dependent variable (outcome). It assumes a linear relationship of the form:\n",
    "\n",
    "```\n",
    "y = β₀ + β₁x + ε\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **y** = dependent variable (what we're predicting)\n",
    "- **x** = independent variable (predictor)\n",
    "- **β₀** = y-intercept (bias term)\n",
    "- **β₁** = slope (weight/coefficient)\n",
    "- **ε** = error term (residual)\n",
    "\n",
    "#### Key Concepts\n",
    "\n",
    "**Least Squares Method**: Finds the best-fitting line by minimizing the sum of squared residuals:\n",
    "```\n",
    "RSS = Σᵢ(yᵢ - ŷᵢ)²\n",
    "```\n",
    "\n",
    "**Coefficient Interpretation**:\n",
    "- **Slope (β₁)**: For each unit increase in x, y changes by β₁ units\n",
    "- **Intercept (β₀)**: Expected value of y when x = 0\n",
    "\n",
    "**Model Assumptions**:\n",
    "- **Linearity**: Relationship between variables is linear\n",
    "- **Independence**: Observations are independent\n",
    "- **Homoscedasticity**: Constant variance of residuals\n",
    "- **Normality**: Residuals are normally distributed\n",
    "\n",
    "#### Performance Metrics\n",
    "\n",
    "**R-squared (R²)**: Proportion of variance explained by the model\n",
    "```\n",
    "R² = 1 - (RSS/TSS)\n",
    "```\n",
    "- Range: 0 to 1 (higher is better)\n",
    "- Interpretation: Percentage of variance explained\n",
    "\n",
    "**Mean Squared Error (MSE)**: Average squared prediction error\n",
    "```\n",
    "MSE = (1/n) Σᵢ(yᵢ - ŷᵢ)²\n",
    "```\n",
    "\n",
    "**Root Mean Squared Error (RMSE)**: Square root of MSE (same units as target)\n",
    "```\n",
    "RMSE = √MSE\n",
    "```\n",
    "\n",
    "### Multiple Regression Basics\n",
    "\n",
    "**Multiple regression** extends simple regression to include multiple independent variables:\n",
    "\n",
    "```\n",
    "y = β₀ + β₁x₁ + β₂x₂ + ... + βₚxₚ + ε\n",
    "```\n",
    "\n",
    "#### Matrix Formulation\n",
    "\n",
    "In matrix notation:\n",
    "```\n",
    "Y = Xβ + ε\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **Y** = n×1 vector of outcomes\n",
    "- **X** = n×(p+1) design matrix (includes intercept column)\n",
    "- **β** = (p+1)×1 vector of coefficients\n",
    "- **ε** = n×1 vector of errors\n",
    "\n",
    "**Normal Equation**: Closed-form solution for coefficients\n",
    "```\n",
    "β̂ = (X^T X)⁻¹ X^T Y\n",
    "```\n",
    "\n",
    "#### Advantages of Multiple Regression\n",
    "\n",
    "**Control for Confounding**: Account for multiple factors simultaneously\n",
    "**Increased Predictive Power**: Often explains more variance than simple regression\n",
    "**Real-world Relevance**: Most phenomena depend on multiple variables\n",
    "\n",
    "#### Additional Considerations\n",
    "\n",
    "**Multicollinearity**: When predictors are highly correlated\n",
    "- **Detection**: Variance Inflation Factor (VIF)\n",
    "- **Solutions**: Remove correlated features, regularization\n",
    "\n",
    "**Feature Selection**: Choosing relevant predictors\n",
    "- **Forward selection**: Start with no variables, add incrementally\n",
    "- **Backward elimination**: Start with all variables, remove incrementally\n",
    "- **Stepwise**: Combination of forward and backward\n",
    "\n",
    "**Regularization Techniques**:\n",
    "- **Ridge regression (L2)**: Penalizes large coefficients\n",
    "- **Lasso regression (L1)**: Can set coefficients to zero (feature selection)\n",
    "- **Elastic Net**: Combines Ridge and Lasso penalties\n",
    "\n",
    "#### Model Validation\n",
    "\n",
    "**Train-Test Split**: Evaluate performance on unseen data\n",
    "**Cross-Validation**: Multiple train-test splits for robust evaluation\n",
    "**Residual Analysis**: Check assumptions through residual plots\n",
    "\n",
    "**Common Diagnostic Plots**:\n",
    "- **Residuals vs. Fitted**: Check homoscedasticity and linearity\n",
    "- **Q-Q Plot**: Assess normality of residuals\n",
    "- **Residuals vs. Leverage**: Identify influential observations\n",
    "\n",
    "Regression analysis provides the foundation for understanding relationships in data and forms the basis for more advanced machine learning techniques. The mathematical principles we've explored (linear algebra for matrix operations, calculus for optimization) directly apply to solving regression problems efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9b38ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression Analysis \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('customer_purchase_dataset_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17049dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a470705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Linear Regression - Annual Income vs Purchase Amount\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Define features and target variable Annual Income vs Purchase Amount\n",
    "X = df[['Annual Income (USD)']]\n",
    "y = df['Purchase Amount (USD)']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Create and train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6f85a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_test, y_test, color='blue', label='Actual Data')\n",
    "plt.plot(X_test, y_pred, color='red', linewidth=2, label='Regression Line')\n",
    "plt.xlabel('Annual Income (USD)')\n",
    "plt.ylabel('Purchase Amount (USD)')\n",
    "plt.title('Linear Regression: Annual Income vs Purchase Amount')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
